# Optimization
![1_-_2dKCQHh_-_Long_Valley](https://user-images.githubusercontent.com/85587286/187789428-26e331b4-403f-41c5-ba4b-8ff04e16aded.gif)

## **_Resources._** ðŸ‘Œ 

 

### **_Read or watch:_**  ðŸ‘ˆ


>> * [Hyperparameter (machine learning)](https://intranet.hbtn.io/rltoken/1QBGvpFW16wWkdzpbPQ3DQ)
>> * [Feature scaling](https://intranet.hbtn.io/rltoken/w-mu-1FTMnCw_bo51x1H1w)
>> * [Why, How and When to Scale your Features](Why, How and When to Scale your Features)
>> * [Normalizing your data](Normalizing your data)
>> * [Moving average](https://intranet.hbtn.io/rltoken/_Na-3oh6JT9YqhWnxE5cRg)
>> * [An overview of gradient descent optimization algorithms](https://intranet.hbtn.io/rltoken/-TMwJwWHSavWMohuQ5yQvA)
>> * [A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size](https://intranet.hbtn.io/rltoken/-Bpr2w5FmPvMnmn-EHW6Rw)
>> * [Stochastic Gradient Descent with momentum](https://intranet.hbtn.io/rltoken/0IV-Ha-0L7AbVSDds_MxAw)
>> * [Understanding RMSprop](https://intranet.hbtn.io/rltoken/s1HPEM3JfB9iIU08zRg8JA)
>> * [Adam](https://intranet.hbtn.io/rltoken/c2ewV4zkooL1VtbAlKygGQ)
>> * [Learning Rate Schedules](https://intranet.hbtn.io/rltoken/K9Fl3Z1Axae8TJfbd1x9Vw)
>> * [deeplearning.ai](https://intranet.hbtn.io/rltoken/-tmvgOpWb_sjJzR5hv6VyA)
>> * [Exponentially Weighted Averages](https://intranet.hbtn.io/rltoken/5N75PDrSPlBuQEXyV5lTuw)
>> * [Understanding Exponentially Weighted Averages](https://intranet.hbtn.io/rltoken/V1fGt--3DYdXIlFaZKxQ1Q)
>> * [Bias Correction of Exponentially Weighted Averages](https://intranet.hbtn.io/rltoken/F4Of4Km8QRl2mH6iCdGReg)
>> * [Gradient Descent With Momentum](https://intranet.hbtn.io/rltoken/DwaovproRxolK5BN2LTbQQ)
>> * [RMSProp](https://intranet.hbtn.io/rltoken/knRX814HFUQcumxnOOJSyw)
>> * [Adam Optimization Algorithm](https://intranet.hbtn.io/rltoken/c9O01hgfn3335zzQPGtlkQ)
>> * [Learning Rate Decay](https://intranet.hbtn.io/rltoken/PXmH63ae5SdNBSvwZOcN5w)
>> * [Normalizing Activations in a Network](https://intranet.hbtn.io/rltoken/bbhczA5i6hu1KC1SVFZf1g)
>> * [Fitting Batch Norm Into Neural Networks](https://intranet.hbtn.io/rltoken/tjvojWwSp0hhFontTO7ygw)
>> * [Why Does Batch Norm Work?](https://intranet.hbtn.io/rltoken/14HrGT4EmpD5lhQThvFOhg)
>> * [Batch Norm At Test Time](https://intranet.hbtn.io/rltoken/RQob4hYaNfjmDDeW7j49bA)
>> * [The Problem of Local Optima](https://intranet.hbtn.io/rltoken/mHsAE3RUtXZ0UQTOgtab9A)
### **_References:_**  ðŸ‘ˆ

>> * [numpy.random.permutation](https://intranet.hbtn.io/rltoken/HRwmVKUVZQCeC1F2FKg4Lg)
>> * [tf.nn.moments](https://intranet.hbtn.io/rltoken/2ONhdmNrX9cDldv4zG60rg)
>> * [tf.train.MomentumOptimizer](https://intranet.hbtn.io/rltoken/nw0eT5r9r_-OAeL2Dpy4Zw)
>> * [tf.train.RMSPropOptimizer](https://intranet.hbtn.io/rltoken/vckCzM32lR3Vv628QX15QA)
>> * [tf.train.AdamOptimizer](https://intranet.hbtn.io/rltoken/jXlYQ-sWHBZuR4gBJF3zpw)
>> * [tf.nn.batch_normalization](https://intranet.hbtn.io/rltoken/J-jO3nO1WOjHg4VwFm_1EA)
>> * [tf.train.inverse_time_decay](https://intranet.hbtn.io/rltoken/q4U2mGTPUICpEPT5Iuwb7w)


## Testing

>> * [graph.ckpt.data-00000-of-00001](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/graph.ckpt.data-00000-of-00001)
>> * [graph.ckpt.index](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/graph.ckpt.index)
>> * [graph.ckpt.meta](https://s3.amazonaws.com/intranet-projects-files/holbertonschool-ml/graph.ckpt.meta)

## **_Built with:_** ðŸ› ï¸

>> * Ubuntu 20.04 LTS
>> 
>> * Emacs editor && Pycharm
>> 
>> * numpy (version 1.19.2) 
>> 
>> * Tensorflow 2.6
>>
>> * pycodestyle (version 2.6)
